{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Runner notebook for Google Colab\n",
        "\n",
        "This notebook uses to run the project py files on Google Colab.<br>\n",
        "You only need to set up `HF_TOKEN` at the relevant section and run all the cells.<br>\n",
        "Change the last cell [Experiment Example] to run any experiment you want!<br>\n",
        "\n",
        "⚠️ Warning!<br>\n",
        "During our experiments we ran 7-8B models with a single `NVIDIA A100-SXM4-40GB` GPU device.<br>\n",
        "Please make sure you have a proper GPU for any other configuration.<br>\n",
        "If you want to run a distributed experiment, plese make sure to set `num_gpus` variable properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4ywZetC3NQB"
      },
      "source": [
        "Clonning `GalSarid21/deep-generative-models-biu` github repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c156e99avFYC",
        "outputId": "b54b3b19-2aa4-4fdd-e9d2-8da4496554ef"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/GalSarid21/deep-generative-models-biu.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIxD-xAe3TkC"
      },
      "source": [
        "Navigating into the `deep-generative-models-biu` folder and clonning `nelson-liu/lost-in-the-middle` github repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig-jUGo7vJ6T",
        "outputId": "c884e827-1f8b-4d62-a97f-9bc4ff3b4a34"
      },
      "outputs": [],
      "source": [
        "%cd /content/deep-generative-models-biu\n",
        "!git clone https://github.com/nelson-liu/lost-in-the-middle.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq8BA9yv3rg6"
      },
      "source": [
        "Installing `requirements.txt` file of the `deep-generative-models-biu` folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBwNaQSGvkTG",
        "outputId": "6100f72f-c2f6-4862-ff05-c848afee97c5"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k718-ZDN34fn"
      },
      "source": [
        "Setting `HF_TOKEN` environment variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iB8IT-RB678u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"<your-hf-token>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iSQzESf4BMN"
      },
      "source": [
        "**Experiment running example:**<br>\n",
        "Here we run a `gold_idx_change` experiment with the `tiiuae/Falcon3-Mamba-7B-Instruct` model [default]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZSK_cABtNbP",
        "outputId": "7c24045e-6b51-40c4-b32a-cc27d17e9c9c"
      },
      "outputs": [],
      "source": [
        "!python main.py --experiment gold_idx_change --model tiiuae/Falcon3-Mamba-7B-Instruct"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
