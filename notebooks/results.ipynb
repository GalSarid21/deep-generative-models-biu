{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9d6bd6",
   "metadata": {},
   "source": [
    "# Experiments Results Analysis\n",
    "\n",
    "In the following notebook we will analyze the preformance of a single SSM (Falcon3-Mamba-7B-Instruct) and a single LLM (Llama-3.1-8B-Instruct).<br>\n",
    "Those are two instruction models with similar sizes (7.27B vs 8.03B params).<br>\n",
    "For each model we'll examine the following:\n",
    "* `gold_index_change` task with 10,20 and 30 documents AND will compare the `openbook_random` and `closedbook` prompting modes.\n",
    "* `num_docs_change` with gold index of 0,4 and 9 AND will compare the `openbook_random` and `closedbook` prompting modes.\n",
    "\n",
    "After those are done - we'll compare the SSM and LLM performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1eae7f",
   "metadata": {},
   "source": [
    "## Utils\n",
    "\n",
    "* matplotlib plots handling\n",
    "* test results statistics calculation\n",
    "* test result object loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7653d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d9cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_results(results_file_path: str) -> Dict[str, Any]:\n",
    "    with open(results_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b163c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_mean(scores: List[float], precision: Optional[int] = 4) -> float:\n",
    "    if precision < 0:\n",
    "        raise ValueError(\"'precision' can't be a negative number.\")\n",
    "    return round(statistics.mean(scores), precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02dbee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_over_documents(\n",
    "    x_values: List[int],\n",
    "    y_values_list: List[List[float]],\n",
    "    series_labels: List[str],\n",
    "    title: Optional[str] = \"Metric over Documents\",\n",
    "    xtitle: Optional[str] = \"Gold Index\",\n",
    "    ytitle: Optional[str] = \"Metric Mean\",\n",
    "    closedbook_mean: Optional[float] = None,\n",
    "    figsize: Optional[Tuple[int, int]] = (10, 6)\n",
    ") -> None:\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for y_values, label in zip(y_values_list, series_labels):\n",
    "        plt.plot(x_values, y_values, marker=\"o\", label=label)\n",
    "\n",
    "    if closedbook_mean is not None and len(y_values_list) == 1:\n",
    "        plt.axhline(\n",
    "            y=closedbook_mean,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"closedbook\"\n",
    "        )\n",
    "\n",
    "    plt.xticks(np.arange(min(x_values), max(x_values)+1, 2))\n",
    "\n",
    "    plt.title(title, weight=\"bold\")\n",
    "    plt.xlabel(xtitle)\n",
    "    plt.ylabel(ytitle)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdb16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_plot_data_lists(\n",
    "    data: Dict[str, Any],\n",
    "    scores_eval: List[float],\n",
    "    x_values: Optional[List[int]] = None,\n",
    "    num_prompt_tokens: Optional[List[int]] = None,\n",
    "    sort_keys: Optional[bool] = True\n",
    ") -> str:\n",
    "\n",
    "    metric = \"\"\n",
    "    keys = list(data[\"experiments\"].keys())\n",
    "    if sort_keys is True:\n",
    "        keys = sorted(keys, key=lambda x: [int(xi) for xi in x.split(\"_\") if xi.isdigit()][0])\n",
    "\n",
    "    for key in keys:\n",
    "        if \"gold_at_\" in key:\n",
    "            str_to_replace = \"gold_at_\"\n",
    "        elif \"_total_documents\" in key:\n",
    "            str_to_replace = \"_total_documents\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unidentified results dict key {key}\")\n",
    "\n",
    "        if x_values is not None:\n",
    "            x_val = int(key.replace(str_to_replace, \"\"))\n",
    "            x_values.append(x_val)\n",
    "\n",
    "        item = data[\"experiments\"][key]\n",
    "        mean = get_scores_mean(item[\"scores\"])\n",
    "        scores_eval.append(mean)\n",
    "\n",
    "        if num_prompt_tokens is not None:\n",
    "            num_prompt_tokens.extend(item[\"num_prompt_tokens\"])\n",
    "\n",
    "        if metric == \"\":\n",
    "            metric = item[\"metric\"]\n",
    "        else:\n",
    "            if item[\"metric\"] != metric:\n",
    "                raise Exception(\"Inconsistent results scores metric\")\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1cb559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_model_results_data(\n",
    "    openbook_path: str,\n",
    "    closedbook_path: Optional[str] = None\n",
    ") -> None:\n",
    "\n",
    "    openbook_data = load_test_results(openbook_path)\n",
    "    openbook_scores_eval = []\n",
    "    num_prompt_tokens = []\n",
    "    x_values = []\n",
    "\n",
    "    metric = populate_plot_data_lists(\n",
    "        data=openbook_data,\n",
    "        x_values=x_values,\n",
    "        scores_eval=openbook_scores_eval,\n",
    "        num_prompt_tokens=num_prompt_tokens\n",
    "    )\n",
    "\n",
    "    closedbook_mean = None\n",
    "    if closedbook_path is not None:\n",
    "        closedbook_data = load_test_results(closedbook_path)\n",
    "        closedbook_scores = closedbook_data[\"experiments\"][\"closedbook\"][\"scores\"]\n",
    "        closedbook_mean = get_scores_mean(closedbook_scores)\n",
    "\n",
    "    # cuts the HuggingFace repo and leaves only the model name\n",
    "    short_model_name = openbook_data[\"model\"].split(\"/\")[-1]\n",
    "    experiment_type = openbook_data[\"experiment_type\"]\n",
    "    \n",
    "    if experiment_type == \"gold_idx_change\":\n",
    "        num_docs = openbook_data[\"num_documents\"]\n",
    "        graph_title = f\"{short_model_name}\\n\" \\\n",
    "            + f\"{experiment_type} with {num_docs} total documents, \" \\\n",
    "            + f\"Avg prompt length: {round(statistics.mean(num_prompt_tokens), 2)} tokens\"\n",
    "    else:\n",
    "        gold_idx = openbook_data[\"gold_index\"]\n",
    "        graph_title = f\"{short_model_name}\\n\" \\\n",
    "            + f\"{experiment_type} with gold index {gold_idx}\"\n",
    "    ytitle = f\"Performance [metric: {metric}]\"\n",
    "\n",
    "    plot_metric_over_documents(\n",
    "        x_values=x_values,\n",
    "        y_values_list=[openbook_scores_eval],\n",
    "        series_labels=[short_model_name.lower()],\n",
    "        title=graph_title,\n",
    "        xtitle=(\n",
    "            \"Gold Index [document with answer]\"\n",
    "            if experiment_type == \"gold_idx_change\"\n",
    "            else \"Number of Documents\" \n",
    "        ),\n",
    "        ytitle=ytitle,\n",
    "        closedbook_mean=closedbook_mean\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c21a9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models_data_consistency(data: List[Any], param_name: str) -> None:\n",
    "    if data[0] != data[-1]:\n",
    "        raise Exception(f\"Can't compare two model performance of different {param_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0958c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_models_comparison(openbook_paths: List[str]) -> None:\n",
    "    \n",
    "    openbook_datas = [load_test_results(openbook_path) for openbook_path in openbook_paths]\n",
    "    openbook_scores_evals = [[], []]\n",
    "    x_values = []\n",
    "    metrics = []\n",
    "\n",
    "    for i in range(2):\n",
    "        metric = populate_plot_data_lists(\n",
    "            data=openbook_datas[i],\n",
    "            x_values=x_values if i == 0 else None,\n",
    "            scores_eval=openbook_scores_evals[i]\n",
    "        )\n",
    "        metrics.append(metric)\n",
    "\n",
    "    validate_models_data_consistency(data=metrics, param_name=\"metrics\")\n",
    "\n",
    "    short_model_names = []\n",
    "    experiment_types = []\n",
    "    num_docs_list = []\n",
    "    for openbook_data in openbook_datas:\n",
    "        # cuts the HuggingFace repo and leaves only the model name\n",
    "        short_model_names.append(openbook_data[\"model\"].split(\"/\")[-1])\n",
    "        experiment_types.append(openbook_data[\"experiment_type\"])\n",
    "        num_docs_list.append(openbook_data[\"num_documents\"])\n",
    "\n",
    "    validate_models_data_consistency(data=experiment_types, param_name=\"experiments\")\n",
    "    validate_models_data_consistency(data=num_docs_list, param_name=\"number of documents\")\n",
    "\n",
    "\n",
    "    graph_title = f\"{short_model_names[0]} vs {short_model_names[-1]}\\n\" \\\n",
    "        + f\"{experiment_types[0]} with {num_docs_list[0]} total documents\"\n",
    "    ytitle = f\"Performance [metric: {metrics[0]}]\"\n",
    "\n",
    "    plot_metric_over_documents(\n",
    "        x_values=x_values,\n",
    "        y_values_list=openbook_scores_evals,\n",
    "        series_labels=short_model_names,\n",
    "        title=graph_title,\n",
    "        xtitle=\"Gold Index [document with answer]\",\n",
    "        ytitle=ytitle\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e9a1",
   "metadata": {},
   "source": [
    "### Utils tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = list(Path(\"./results\").rglob(f\"*json\"))[0]\n",
    "results = load_test_results(test_file_path)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a657471",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores_mean(results[\"experiments\"][\"gold_at_0\"][\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b831f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_over_documents([0, 4, 9], [[0.6, 0.4, 0.5]], [\"falcon-mamaba\"], closedbook_mean=0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_over_documents([0, 4, 9], [[0.6, 0.4, 0.5], [0.67, 0.58, 0.61]], [\"falcon-mamaba\", \"llama-31\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94a30b",
   "metadata": {},
   "source": [
    "## Falcon3-Mamba-7B-Instruct Analysis (SSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757ce15",
   "metadata": {},
   "source": [
    "### Gold Index change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35ff227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_closedbook_path = \"../results/Falcon3-Mamba-7B-Instruct/gold_idx_change_experiment/closedbook_prompting\"\n",
    "falcon_closedbook_file = list(Path(falcon_closedbook_path).glob(f\"*json\"))[0]\n",
    "falcon_openbook_path = \"../results/Falcon3-Mamba-7B-Instruct/gold_idx_change_experiment/openbook_prompting_mode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b588740",
   "metadata": {},
   "source": [
    "10 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af995003",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_openbook_file_10_docs = list(Path(f\"{falcon_openbook_path}/10_docs\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=falcon_openbook_file_10_docs,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa94bd",
   "metadata": {},
   "source": [
    "20 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_openbook_file_20_docs = list(Path(f\"{falcon_openbook_path}/20_docs\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=falcon_openbook_file_20_docs,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f98f7c",
   "metadata": {},
   "source": [
    "30 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_openbook_file_30_docs = list(Path(f\"{falcon_openbook_path}/30_docs\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=falcon_openbook_file_30_docs,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f8938",
   "metadata": {},
   "source": [
    "### Input Documents Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9fe42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_openbook_path = \"../results/Falcon3-Mamba-7B-Instruct/num_docs_change_experiment/openbook_prompting_mode\"\n",
    "falcon_closedbook_path = \"../results/Falcon3-Mamba-7B-Instruct/gold_idx_change_experiment/closedbook_prompting\"\n",
    "falcon_closedbook_file = list(Path(falcon_closedbook_path).glob(f\"*json\"))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4422d57",
   "metadata": {},
   "source": [
    "gold at index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_openbook_file_gold_idx_0 = list(Path(f\"{falcon_openbook_path}/gold_idx_0\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=falcon_openbook_file_gold_idx_0,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba60709",
   "metadata": {},
   "source": [
    "gold at index 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_openbook_file_gold_idx_4 = list(Path(f\"{falcon_openbook_path}/gold_idx_4\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=falcon_openbook_file_gold_idx_4,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd2a79",
   "metadata": {},
   "source": [
    "gold at index 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ca64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_openbook_file_gold_idx_9 = list(Path(f\"{falcon_openbook_path}/gold_idx_9\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=falcon_openbook_file_gold_idx_9,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4ac6b",
   "metadata": {},
   "source": [
    "## Llama-3.1-8B-Instruct Analysis (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e5f98",
   "metadata": {},
   "source": [
    "### Gold Index Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b9eca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_closedbook_path = \"./results/Llama-3.1-8B-Instruct/gold_idx_change_experiment/closedbook_prompting\"\n",
    "llama_closedbook_file = list(Path(llama_closedbook_path).glob(f\"*json\"))[0]\n",
    "llama_openbook_path = \"./results/Llama-3.1-8B-Instruct/gold_idx_change_experiment/openbook_prompting_mode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194e1a1",
   "metadata": {},
   "source": [
    "10 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_openbook_file_10_docs = list(Path(f\"{llama_openbook_path}/10_docs\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=llama_openbook_file_10_docs,\n",
    "    closedbook_path=llama_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280baa2",
   "metadata": {},
   "source": [
    "20 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_openbook_file_20_docs = list(Path(f\"{llama_openbook_path}/20_docs\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=llama_openbook_file_20_docs,\n",
    "    closedbook_path=llama_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a19ba1",
   "metadata": {},
   "source": [
    "30 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_openbook_file_30_docs = list(Path(f\"{llama_openbook_path}/30_docs\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=llama_openbook_file_30_docs,\n",
    "    closedbook_path=llama_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594cc9b2",
   "metadata": {},
   "source": [
    "### Input Documents Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37ef1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_openbook_path = \"../results/Llama-3.1-8B-Instruct/num_docs_change_experiment/openbook_prompting_mode\"\n",
    "llama_closedbook_path = \"../results/Llama-3.1-8B-Instruct/gold_idx_change_experiment/closedbook_prompting\"\n",
    "llama_closedbook_file = list(Path(llama_closedbook_path).glob(f\"*json\"))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52581c4",
   "metadata": {},
   "source": [
    "gold at index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_openbook_file_gold_idx_0 = list(Path(f\"{llama_openbook_path}/gold_idx_0\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=llama_openbook_file_gold_idx_0,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d3aae",
   "metadata": {},
   "source": [
    "gold at index 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18994079",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_openbook_file_gold_idx_4 = list(Path(f\"{llama_openbook_path}/gold_idx_4\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=llama_openbook_file_gold_idx_4,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71b722",
   "metadata": {},
   "source": [
    "gold at index 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_openbook_file_gold_idx_9 = list(Path(f\"{llama_openbook_path}/gold_idx_9\").glob(f\"*json\"))[0]\n",
    "plot_single_model_results_data(\n",
    "    openbook_path=llama_openbook_file_gold_idx_9,\n",
    "    closedbook_path=falcon_closedbook_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c321f",
   "metadata": {},
   "source": [
    "## Falcon - Llama Comparison (SSM vs LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9436a7",
   "metadata": {},
   "source": [
    "### Gold Index Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2da0ba",
   "metadata": {},
   "source": [
    "10 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b446a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_models_comparison([falcon_openbook_file_10_docs, llama_openbook_file_10_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb037fc",
   "metadata": {},
   "source": [
    "20 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_models_comparison([falcon_openbook_file_20_docs, llama_openbook_file_20_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819dd2c",
   "metadata": {},
   "source": [
    "30 total documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_models_comparison([falcon_openbook_file_30_docs, llama_openbook_file_30_docs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-generative-models-biu--ZzTfCAn-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
